{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pathlib\n",
    "import types\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, log_loss, jaccard_score, matthews_corrcoef, precision_recall_fscore_support, hamming_loss, cohen_kappa_score, roc_auc_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preformance\n",
    "global_metrics = [\n",
    "    \"Support\", \n",
    "    \n",
    "    \"Accuracy\",\n",
    "    \n",
    "    \"binary\",   #\n",
    "    \"macro\",\n",
    "    \"micro\",\n",
    "    \"weighted\",\n",
    "    \n",
    "    \"Log_Loss\", #\n",
    "    \"Jaccard_Score\",\n",
    "    \"Metrics_Matthews_Corrcoef\",\n",
    "    \"Cohen_Kappa_Score\",\n",
    "    \"Hamming_Loss\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def model_evalution(model_name: str, label_: np.ndarray, pred_: np.ndarray, metrics_: list[str]=None) -> dict:\n",
    "    results = {}\n",
    "    \n",
    "    if pred_.ndim==2:\n",
    "        pred_cat = pred_\n",
    "        pred = np.argmax(pred_, axis=1)\n",
    "        if label_.ndim==1:\n",
    "            label_cat = to_categorical(label_)\n",
    "            label = label_\n",
    "        else:\n",
    "            label_cat = label_\n",
    "            label = np.argmax(label_, axis=1)\n",
    "    else:\n",
    "        label = label_\n",
    "        pred = pred_\n",
    "        \n",
    "    metrics = metrics_ or global_metrics\n",
    "\n",
    "    if \"binary\" in metrics:\n",
    "        metrics.remove(\"binary\")\n",
    "        if metrics_:\n",
    "            print(\"Warning: binary is removed from metrics, as it is only applicable to binary classification\")\n",
    "    if pred_.ndim==1:\n",
    "        metrics.remove(\"Log_Loss\")\n",
    "        if metrics_:\n",
    "            print(\"Warning: Log_Loss is removed from metrics, as it is only applicable if pred_ is 2D\")\n",
    "        \n",
    "    if \"Support\" in metrics:\n",
    "        results[\"Support\"] = len(label)\n",
    "    if \"Accuracy\" in metrics:\n",
    "        results[\"Accuracy\"] = accuracy_score(label, pred)\n",
    "    if \"binary\" in metrics:\n",
    "        p, r, f = precision_recall_fscore_support(label, pred, average=\"binary\")\n",
    "        results[\"Precision\"] = p\n",
    "        results[\"Recall\"] = r\n",
    "        results[\"F1_Score\"] = f\n",
    "    if \"macro\" in metrics:\n",
    "        p, r, f = precision_recall_fscore_support(label, pred, average=\"macro\")\n",
    "        results[\"Precision-macro\"] = p\n",
    "        results[\"Recall-macro\"] = r\n",
    "        results[\"F1_Score-macro\"] = f\n",
    "    if \"micro\" in metrics:\n",
    "        p, r, f = precision_recall_fscore_support(label, pred, average=\"micro\")\n",
    "        results[\"Precision-micro\"] = p\n",
    "        results[\"Recall-micro\"] = r\n",
    "        results[\"F1_Score-micro\"] = f\n",
    "    if \"weighted\" in metrics:\n",
    "        p, r, f = precision_recall_fscore_support(label, pred, average=\"weighted\")\n",
    "        results[\"Precision-weighted\"] = p\n",
    "        results[\"Recall-weighted\"] = r\n",
    "        results[\"F1_Score-weighted\"] = f\n",
    "    if \"Log_Loss\" in metrics:\n",
    "        results[\"Log_Loss\"] = log_loss(label_cat, pred_cat)\n",
    "    if \"Jaccard_Score\" in metrics:\n",
    "        results[\"Jaccard_Score\"] = jaccard_score(label, pred)\n",
    "    if \"Metrics_Matthews_Corrcoef\" in metrics:\n",
    "        results[\"Metrics_Matthews_Corrcoef\"] = matthews_corrcoef(label, pred)\n",
    "    if \"Cohen_Kappa_Score\" in metrics:\n",
    "        results[\"Cohen_Kappa_Score\"] = cohen_kappa_score(label, pred)\n",
    "    if \"Hamming_Loss\" in metrics:\n",
    "        results[\"Hamming_Loss\"] = hamming_loss(label, pred)\n",
    "    if \"ROC_AUC_Score\" in metrics:\n",
    "        results[\"ROC_AUC_Score\"] = roc_auc_score(label, pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_model_evalution_df(model_name: str, label_: np.ndarray,  pred_: np.ndarray, metrics_: list[str]=None) -> pd.DataFrame:\n",
    "    results = model_evalution(model_name, label_, pred_)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5a1ae6899980971a482c9ba4350aa5d29248927543f8d54686f28fa951765f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
